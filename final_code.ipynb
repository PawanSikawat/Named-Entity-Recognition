{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIN_COUNT = 20000\n",
    "WIN_SIZE = 5\n",
    "HID_NODE = 6\n",
    "NO_CLASS = 6  #INCLUDING CLASS TYPE NONE\n",
    "CLASS_NAME = [\"TIME\",\"PERSON\",\"GEOGRAPHICAL ENTITY\",\"ORGANISATION\",\"OTHERS\",\"GEOPOLITICAL ENTITY\"]\n",
    "READ_COUNT = 0\n",
    "ONE_TIME = 1000\n",
    "c4c = 0\n",
    "weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_HID = 1     #NO OF HIDDEN LAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHOICE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_count():\n",
    "    print(\"\\nPlease choose one of the following:\")\n",
    "    print(\"\\n1. One\")\n",
    "    print(\"\\n2. Three\")\n",
    "    choice = input(\"\\nEnter your choice: \")\n",
    "    if choice == 1:\n",
    "        NO_HID = 1\n",
    "    elif choice == 2:\n",
    "        NO_HID = 3\n",
    "    else:\n",
    "        print(\"\\n\\nERROR: Please choose a valid input\\n\")\n",
    "        layer_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_choice():\n",
    "    print(\"\\nPlease choose one of the following:\")\n",
    "    print(\"\\n1. Use previously trained weights\")\n",
    "    print(\"\\n2. Use new weights\")\n",
    "\n",
    "    choice = input(\"\\nEnter your choice: \")\n",
    "\n",
    "    if choice == 1:\n",
    "    #call a function that uses previous weights\n",
    "        choice = 1 #RANDOM CODE\n",
    "    elif choice == 2:\n",
    "    #call a function that initialises weights as given in assignment\n",
    "        choice = 2 #RANDOM CODE\n",
    "    else:\n",
    "        print(\"\\n\\nERROR: Please choose a valid input\\n\")\n",
    "        weight_choice()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTIVATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "    func = lambda x: x\n",
    "    line=func(z)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    maxi=np.maximum(0,z)\n",
    "    return maxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    part=1+np.exp(-z)\n",
    "    sigm=1/part\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    part=1+np.exp(-2*z)\n",
    "    tan=(2/part)-1\n",
    "    return tan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.1):\n",
    "    lrelu=np.maximum(alpha*z,z)\n",
    "    return lrelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(z):\n",
    "    splus=np.log(1+np.exp(z))\n",
    "    return splus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELU(z, alpha=0.1):\n",
    "    part=alpha*(np.exp(z)-1)\n",
    "    eul=np.where(z>=0,z,part)\n",
    "    return eul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    expA = np.exp(z)\n",
    "    return expA / expA.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONSTRUCTING NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_NN(layers_sizes, rand_w):\n",
    "    H = len(layers_sizes)-1\n",
    "    global weights\n",
    "    for h in range(H):\n",
    "        eh=layers_sizes[h]\n",
    "        eh1=layers_sizes[h+1]\n",
    "        if rand_w == 2:\n",
    "            layer_weights=np.random.uniform(low=0.0, high=0.01,size=eh*eh1).reshape(eh1,eh)\n",
    "        else:\n",
    "            layer_weights=np.random.normal(loc=0.005, scale=0.005,size=eh*eh1).reshape(eh1,eh)  \n",
    "        layer_biases=np.zeros((eh1,1))\n",
    "        weights.append([layer_weights,layer_biases])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FORWARD MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(A_prev, W, b, activation, alpha=0.1):\n",
    "    Z=np.dot(np.array(A_prev).T, np.array(W).T).T+b\n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        A = leaky_relu(Z, alpha)\n",
    "    elif activation == \"softplus\":\n",
    "        A = softplus(Z)\n",
    "    elif activation == \"ELU\":\n",
    "        A = ELU(Z, alpha)\n",
    "    else: # Linear activation\n",
    "        A = linear(Z)\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_module(X, weights, activations, alpha=0.1):\n",
    "    layers_cache = []\n",
    "    H = len(weights)\n",
    "    A = X\n",
    "    for h in range(H):\n",
    "        if h!=0:\n",
    "            B=A\n",
    "            A,Y=forward_step(A,weights[h][0],weights[h][1],activations[h],alpha)\n",
    "            layers_cache.append([B,weights[h][0:1],Y])\n",
    "        else:\n",
    "            A,Y=forward_step(X,weights[h][0],weights[h][1],activations[h],alpha)\n",
    "            layers_cache.append([X,weights[h][0:1],Y])\n",
    "    \n",
    "    return A, layers_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COST FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_cost(AH, Y,reg):\n",
    "    cost=0\n",
    "    m = len(AH[0])\n",
    "    lambd = 10.0\n",
    "    if reg == 1:\n",
    "        cost =  -(1.0/m) * (np.dot(np.log(AH), Y) + np.dot(np.log(1-AH), (1-Y)))\n",
    "    else:\n",
    "        cost = (np.sum(np.square(W1)) + np.sum(np.square(W2)))*(lambd/(2*m))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(prediction, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    predictions = prediction.T\n",
    "    N = predictions.shape[0]\n",
    "    prob = np.zeros((N,NO_CLASS))\n",
    "    for i in range(N):\n",
    "        prob[i] = softmax(predictions[i])\n",
    "    prob = np.clip(prob, epsilon, 1. - epsilon)\n",
    "    ce = -np.sum(targets*np.log(prob+1e-9),axis=1)/N\n",
    "    return ce.sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy2(prediction, targets, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    Computes cross entropy between targets (encoded as one-hot vectors)\n",
    "    and predictions. \n",
    "    Input: predictions (N, k) ndarray\n",
    "           targets (N, k) ndarray        \n",
    "    Returns: scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = prediction.T\n",
    "    N = predictions.shape[0]\n",
    "    ce = np.zeroes(N)\n",
    "    for i in range(N):\n",
    "        right = 0\n",
    "        tw = 0\n",
    "        for j in range(NO_CLASS):\n",
    "            sum = sum + prediction[i][j]*targets[i][j]\n",
    "            tw = tw\n",
    "        ce[i] = sum\n",
    "    prob = np.zeros((N,NO_CLASS))\n",
    "    for i in range(N):\n",
    "        prob[i] = softmax(predictions[i])\n",
    "    prob = np.clip(prob, epsilon, 1. - epsilon)\n",
    "    ce = -np.sum(targets*np.log(prob+1e-9),axis=1)/N\n",
    "    return ce.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTIVATIONS BACKWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dA, Z):\n",
    "    dZ=dA\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, Z):\n",
    "    dZ=dA*np.where(Z>=0,1,0)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, Z):\n",
    "    sigm=sigmoid(Z)\n",
    "    dZ=dA*sigm*(1-sigm)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_backward(dA, Z):\n",
    "    tansq=tanh(Z)*tanh(Z)\n",
    "    dZ=dA*(1-tansq)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_backward(dA, Z, alpha=0.1):\n",
    "    dZ=dA*np.where(Z>=0,1,alpha)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus_backward(dA, Z):\n",
    "    dZ=dA*sigmoid(Z)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELU_backward(dA, Z, alpha=0.1):\n",
    "    part=alpha*np.exp(Z)\n",
    "    dZ=dA*np.where(Z>=0,1,part)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(dA, Z):\n",
    "      dZ=dA*softmax(Z)*(1-softmax(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_backward(dA, Z, activation, alpha=0.1):\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, Z)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "    elif activation == \"tanh\":\n",
    "        dZ = tanh_backward(dA, Z)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        dZ = leaky_relu_backward(dA, Z, alpha)\n",
    "    elif activation == \"softplus\":\n",
    "        dZ = softplus_backward(dA, Z)\n",
    "    elif activation == \"ELU\":\n",
    "        dZ = ELU_backward(dA, Z, alpha)\n",
    "    else: # Linear activation\n",
    "        dZ = linear_backward(dA, Z)   \n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BACKWARD MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_step(dA, layer_cache, activation, alpha=0.1):\n",
    "    A_prev = np.array(layer_cache[0])\n",
    "    W = np.array(layer_cache[1])\n",
    "    Z = np.array(layer_cache[2])\n",
    "    dZ=activation_backward(dA,Z,activation,alpha)\n",
    "    _,m=dA.shape\n",
    "    dA_prev=np.dot(np.squeeze(W).T,dZ)\n",
    "    dW=(np.dot(dZ,A_prev.T)) \n",
    "    dW = dW/m\n",
    "    db=np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_module(AH, Y, layers_cache, activations, alpha=0.1):\n",
    "    gradients = []\n",
    "    H = len(layers_cache) # the number of layers\n",
    "    m = AH.shape[1]\n",
    "    Y = Y.T\n",
    "    dAH=(np.divide(1-Y,1-AH)-np.divide(Y,AH))\n",
    "    for i in range(H-1,-1,-1):\n",
    "        dAH,dW,db=backward_step(dAH,layers_cache[i],activations[i],alpha)\n",
    "        gradients.insert(i,[dW,db])\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(weights, gradients, lr):\n",
    "    H = len(weights)\n",
    "    for h in range(H):\n",
    "        weights[h][0]=weights[h][0]-lr*gradients[h][0]\n",
    "        weights[h][1]=weights[h][1]-lr*gradients[h][1]   \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_layer_NN(X, Y, W, layers_sizes, activations, epochs=200, lr=0.001, alpha=0.1):\n",
    "    epoch_wise_costs = []  \n",
    "    weights = W\n",
    "    global c4c\n",
    "    pos4 = []\n",
    "    npx = np.asarray(X)\n",
    "\n",
    "    for i in range(Y.shape[0]):\n",
    "        if Y[i][4] == 1 and c4c!=14:\n",
    "            pos4.append(i)\n",
    "            c4c = c4c + 1\n",
    "        elif c4c == 14:\n",
    "            c4c = 0\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    X2 = np.delete(X,pos4,1)\n",
    "    Y2 = np.delete(Y,pos4,0)\n",
    "\n",
    "    count_class = np.sum(Y2,axis=0)\n",
    "            \n",
    "    for epoch in range(epochs):\n",
    "        AH, layers_cache = forward_module(X2, weights, activations, alpha)\n",
    "        cost = cross_entropy(AH, Y2)\n",
    "        gradients = backward_module(AH, Y2, layers_cache, activations, alpha)\n",
    "        twice = 1       \n",
    "        update_weights(weights, gradients, lr)\n",
    "        epoch_wise_costs.append(cost)\n",
    "        print(\".\\nCost after epoch: %f\" %(cost))\n",
    "\n",
    "    return weights,cost,count_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORD VECTORS READER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict() #global variable\n",
    "\n",
    "f = open(\"glove.6B.50d.txt\",encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMBEDDING LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(skiprow,nrow):\n",
    "    with open(\"ner.csv\",\"r\") as f:\n",
    "        \n",
    "        data = pd.read_csv (f,skiprows=skiprow,nrows=nrow)   \n",
    "        df = pd.DataFrame(data, columns= ['prev-prev-word','prev-word','word','next-word','next-next-word','prev-prev-iob','prev-iob','tag','prev-prev-shape','prev-shape','shape','next-shape','next-next-shape'])\n",
    "    \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_class(word):\n",
    "    if word == 'B-per' or word == 'I-per':\n",
    "        return 1\n",
    "    elif word == 'B-geo' or word == 'I-geo':\n",
    "        return 2\n",
    "    elif word == 'B-org' or word == 'I-org':\n",
    "        return 3\n",
    "    elif word == 'B-gpe' or word == 'I-gpe':\n",
    "        return 5\n",
    "    elif word == 'B-tim' or word == 'I-tim':\n",
    "        return 0\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer_output(training_example, window_size, isImprovised, improvedVector, prev_tagged):\n",
    "    \"\"\"\"\n",
    "    assuming training example will be a list of 5 words\n",
    "    if isImprovised is true then the 7 inputs are also passed as a vector. First 5 entries are 0 if lowercase and 1 if upper case for each word. Last two entries are corresponding to tag\n",
    "    tag of previous two words will have to be made into categorical type: 0,1,2,3,4,5... corresponding to each tag\n",
    "     training eg should be of the form: [prev-prev, prev, word, next, next-next]\n",
    "    assuming window size will be either 0,1, or 2\n",
    "    \"\"\"\n",
    "    output = np.zeros(257)\n",
    "    if(isImprovised):\n",
    "        output[252:257] = improvedVector\n",
    "        output[250:252] = prev_tagged\n",
    "    temp = embeddings_index.get(training_example[2].lower())\n",
    "    if (training_example[2] in embeddings_index.keys()):\n",
    "        output[100:150] = temp\n",
    "    if(window_size>0):\n",
    "        temp = embeddings_index.get(training_example[1].lower())\n",
    "        if (training_example[1] in embeddings_index.keys()):\n",
    "            output[50:100] = temp\n",
    "        temp = embeddings_index.get(training_example[3].lower())\n",
    "        if (training_example[3] in embeddings_index.keys()):\n",
    "             output[150:200] = temp\n",
    "        if(window_size>1):\n",
    "            temp = embeddings_index.get(training_example[0].lower())\n",
    "            if (training_example[0] in embeddings_index.keys()):\n",
    "                output[50:100] = temp\n",
    "            temp = embeddings_index.get(training_example[4].lower())\n",
    "            if (training_example[4] in embeddings_index.keys()):\n",
    "                output[150:200] = temp\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(df,sub):\n",
    "    window_matrix = []\n",
    "    for ind in df.index: \n",
    "        window_matrix.append([df['prev-prev-word'][ind], df['prev-word'][ind], df['word'][ind], df['next-word'][ind], df['next-next-word'][ind]])\n",
    "  \n",
    "    shape_matrix = []\n",
    "    prev_tag = []\n",
    "    zero = [0,0,0,0,0]\n",
    "    single = zero\n",
    "    for ind in df.index:\n",
    "        single = zero\n",
    "        if df['prev-prev-shape'][ind] == 'capitalized':\n",
    "            single[0] = 1\n",
    "        if df['prev-shape'][ind] == 'capitalized':\n",
    "            single[1] = 1\n",
    "        if df['shape'][ind] == 'capitalized':\n",
    "            single[2] = 1\n",
    "        if df['next-shape'][ind] == 'capitalized':\n",
    "            single[3] = 1\n",
    "        if df['next-next-shape'][ind] == 'capitalized':\n",
    "            single[4] = 1\n",
    "        prev_tag.append([give_class(df['prev-prev-iob'][ind]), give_class(df['prev-iob'][ind])])\n",
    "        shape_matrix.append(single)\n",
    "\n",
    "    input_matrix = []\n",
    "    for ind in df.index:\n",
    "        ind = ind - sub\n",
    "        temp = embedding_layer_output(window_matrix[ind],2,True,shape_matrix[ind],prev_tag[ind])\n",
    "        input_matrix.append(temp)\n",
    "    one_hot_labels = np.zeros((len(df.index), NO_CLASS))\n",
    "    class_code = 0\n",
    "    for ind in df.index:\n",
    "        class_code = give_class(df['tag'][ind])\n",
    "        ind = ind - sub\n",
    "        one_hot_labels[ind][class_code] = 1\n",
    "\n",
    "    return input_matrix, one_hot_labels\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cont(layers_sizes,activations,reg,rand_w,wind,epochs):\n",
    "    \n",
    "    print(\"LAYERS SIZES\",layers_sizes)\n",
    "    lr = 0.03\n",
    "    alpha = 0.1\n",
    "    TOTAL_READ_COUNT = 1000\n",
    "    READ_COUNT = 0\n",
    "    ONE_TIME = 1000\n",
    "    chunk = 1000\n",
    "    layers_sizes = [257, 50, 6] #Last one is the number of output classes  &&  First one is the number of input nodes\n",
    "    activations = [\"ELU\",\"sigmoid\"]\n",
    "    data = pd.read_csv('ner.csv', encoding='unicode-escape',chunksize = chunk)\n",
    "    global weights \n",
    "    weights = construct_NN(layers_sizes, rand_w)\n",
    "    ind = 1\n",
    "    cost_arr = []\n",
    "    total_count = np.zeros(NO_CLASS)\n",
    "    for i in range(epochs):\n",
    "        for  c in data:\n",
    "            if (ind == 69 or ind == 690):\n",
    "                ind = ind+1\n",
    "                continue\n",
    "            if (ind==10):\n",
    "                test_set = c\n",
    "                ind = ind + 1\n",
    "                continue\n",
    "            if (ind!=10 and ind%10==0):\n",
    "                test_set.append(c,ignore_index = True)\n",
    "                ind = ind+1\n",
    "                continue      \n",
    "            ind = ind +1\n",
    "            if (ind>100):\n",
    "                break\n",
    "            input_matrix, one_hot_labels = initialise(c,(ind-2)*chunk)\n",
    "            input_transpose = [[row[i] for row in input_matrix] for i in range(len(input_matrix[0]))]\n",
    "            weights,cost,count_class = H_layer_NN(input_transpose, one_hot_labels, weights, layers_sizes, activations, 5, lr, alpha)\n",
    "            cost_arr.append(cost)\n",
    "            total_count = total_count + count_class\n",
    "\n",
    "    print(\"TOTAL COUNT: \",total_count)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,Y,weights,activations,alpha=0.1,total=10500):\n",
    "    confuse_matrix = np.zeros((NO_CLASS,NO_CLASS))\n",
    "    confuse_matrix = np.squeeze(confuse_matrix)\n",
    "    output, _ = forward_module(X, weights, activations, alpha)\n",
    "    output = np.squeeze(output)\n",
    "    output = output.T\n",
    "    pred_class = np.zeros((total,NO_CLASS))\n",
    "    max_val = np.amax(output, axis=1)\n",
    "    actual = 0\n",
    "    predicted = 0\n",
    "\n",
    "    for i in range(total):\n",
    "        predicted = np.asarray(np.where(output[i] == max_val[i]))\n",
    "        pred_class[i][predicted] = 1\n",
    "        actual = np.asarray(np.where(Y[i] == 1))\n",
    "        pred_val = predicted[0][0]\n",
    "        act_val = actual[0][0]\n",
    "        confuse_matrix[act_val][pred_val] = confuse_matrix[act_val][pred_val] + 1\n",
    "  \n",
    "    accurate_pred = 0\n",
    "    for i in range(NO_CLASS):\n",
    "        accurate_pred = accurate_pred + confuse_matrix[i][i]\n",
    "    accuracy = accurate_pred/total\n",
    "    print(\"\\nAccuracy:\", accuracy)\n",
    "\n",
    "    precision = np.zeros(NO_CLASS)\n",
    "    class_total = np.sum(confuse_matrix, axis=0)\n",
    "    for i in range(NO_CLASS):\n",
    "        precision[i] = confuse_matrix[i][i]/(class_total[i]+1)\n",
    "        print(\"\\nF1 Score for class %d[%s] is: %1.3f\" %(i+1,CLASS_NAME[i],precision[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_test(df,sub):\n",
    "    window_matrix = []\n",
    "    print(df.index)\n",
    "    for ind in df.index: \n",
    "        window_matrix.append([df['prev-prev-word'][ind], df['prev-word'][ind], df['word'][ind], df['next-word'][ind], df['next-next-word'][ind]])\n",
    "  \n",
    "    shape_matrix = []\n",
    "    prev_tag = []\n",
    "    zero = [0,0,0,0,0]\n",
    "    single = zero\n",
    "    for ind in df.index:\n",
    "        single = zero\n",
    "        if df['prev-prev-shape'][ind] == 'capitalized':\n",
    "            single[0] = 1\n",
    "        if df['prev-shape'][ind] == 'capitalized':\n",
    "            single[1] = 1\n",
    "        if df['shape'][ind] == 'capitalized':\n",
    "            single[2] = 1\n",
    "        if df['next-shape'][ind] == 'capitalized':\n",
    "            single[3] = 1\n",
    "        if df['next-next-shape'][ind] == 'capitalized':\n",
    "            single[4] = 1\n",
    "        prev_tag.append([give_class(df['prev-prev-iob'][ind]), give_class(df['prev-iob'][ind])])\n",
    "        shape_matrix.append(single)\n",
    "\n",
    "    input_matrix = []\n",
    "    for ind in df.index:\n",
    "        ind = ind - sub\n",
    "        temp = embedding_layer_output(window_matrix[ind],2,True,shape_matrix[ind],prev_tag[ind])\n",
    "        input_matrix.append(temp)\n",
    "    one_hot_labels = np.zeros((len(df.index), NO_CLASS))\n",
    "    class_code = 0\n",
    "    for ind in df.index:\n",
    "        class_code = give_class(df['tag'][ind])\n",
    "        ind = ind - sub\n",
    "        one_hot_labels[ind][class_code] = 1\n",
    "\n",
    "    return input_matrix, one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_test():\n",
    "    for i in range(10):\n",
    "        sf = read_file(i*ONE_TIME*10,ONE_TIME*10)\n",
    "        if i == 0:\n",
    "            tf = sf\n",
    "        else:\n",
    "            tf.append(sf, ignore_index=True)\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cont(wind,activations):\n",
    "    global weights\n",
    "    test_set = give_test()\n",
    "    total = len(test_set.index)\n",
    "    print(\"TOTAL\",total)\n",
    "    input_test, output_test = initialise_test(test_set,0)\n",
    "    test_transpose = [[row[i] for row in input_test] for i in range(len(input_test[0]))]\n",
    "    np_test = np.asarray(test_transpose)\n",
    "    if wind == 2:\n",
    "        np_test[:][0] = 0\n",
    "        np_test[:][1] = 0\n",
    "        np_test[:][3] = 0\n",
    "        np_test[:][4] = 0\n",
    "    elif wind == 1:\n",
    "        np_test[:][0] = 0\n",
    "        np_test[:][4] = 0\n",
    "    else:\n",
    "        xyz = 0  ####RANDOM    can also use PASS\n",
    "    test_transpose = np_test.tolist()\n",
    "    output_test = np.asarray(output_test)\n",
    "    print(output_test.shape)\n",
    "    print(\"LENGTH\",len(test_transpose))\n",
    "    count_class = np.sum(output_test,axis=0)\n",
    "    print(\"COUNT CLASS\",count_class)\n",
    "    predict(test_transpose, output_test, weights, activations, 0.1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN MODULE & CO-FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global weights\n",
    "    weights = []\n",
    "    \n",
    "    hidd_nos = input(\"Choose the number of hidden layers \\nMin = 1 and Max = 3\\n\")\n",
    "    hid_no = int(hidd_nos)\n",
    "    layers_sizes = [257]\n",
    "    if hid_no == 1:\n",
    "        layers_sizes.append(50)\n",
    "    elif hid_no == 2:\n",
    "        layers_sizes.append(50)\n",
    "        layers_sizes.append(50)\n",
    "    elif hid_no == 3:\n",
    "        layers_sizes.append(50)\n",
    "        layers_sizes.append(50)\n",
    "        layers_sizes.append(50)\n",
    "    else:\n",
    "        print(\"WRONG CHOICE OF LAYERS SIZE\")\n",
    "    layers_sizes.append(NO_CLASS)\n",
    "    \n",
    "    print(\"Choose the activation functions for the hidden layers\")\n",
    "    print(\"1.ReLU \\n2.Sigmoid \\n3.TanH \\n4.Leaky ReLU \\n5.Softplus \\n6.ELU \\n7.Linear\")\n",
    "    activations = []\n",
    "    for i in range(hid_no + 1):\n",
    "        print(\"Activation function for layer %d:\",i+1)\n",
    "        take = input(\"Your Choice: \")\n",
    "        if take == \"1\":\n",
    "            activations.append(\"relu\")\n",
    "        elif take == \"2\":\n",
    "            activations.append(\"sigmoid\")\n",
    "        elif take == \"3\":\n",
    "            activations.append(\"tanh\")\n",
    "        elif take == \"4\":\n",
    "            activations.append(\"leaky_relu\")\n",
    "        elif take == \"5\":\n",
    "            activations.append(\"softplus\")\n",
    "        elif take == \"6\":\n",
    "            activations.append(\"ELU\")\n",
    "        else:\n",
    "            activations.append(\"linear\")\n",
    "    \n",
    "    reg_s = input(\"\\nDo you want regularization of cost function? \\nPress 1 for Yes \\nPress 2 for No \\n\")\n",
    "    reg = int(reg_s)\n",
    "    \n",
    "    rand_s = input(\"\\nHow do you want to initialise weights? \\n1.Random \\n2.Uniform \\n\")\n",
    "    rand_w = int(rand_s)\n",
    "    \n",
    "    print(\"\\nYou have the window size choices of 0/1/2\")\n",
    "    wind_s = input(\"Choose your window size: \")\n",
    "    wind = int(wind_s)\n",
    "    wind_l = [wind]\n",
    "    \n",
    "    print(\"\\nYou have the epoch choices between 0-100\")\n",
    "    epoch_s = input(\"Your Choice: \")\n",
    "    epoch = int(epoch_s)\n",
    "    \n",
    "    weights = train_cont(layers_sizes,activations,reg,rand_w,wind,epoch)\n",
    "    \n",
    "    start = [wind_l,layers_sizes,activations]\n",
    "    \n",
    "    # opening the csv file in 'w+' mode \n",
    "    file = open('weights.csv', 'w+', newline ='') \n",
    "\n",
    "    # writing the data into the file \n",
    "    with file:     \n",
    "        write = csv.writer(file) \n",
    "        write.writerows(start)\n",
    "        write.writerows(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    global weights\n",
    "    weights = [[0,0],[0,0]]\n",
    "    ini_list = []\n",
    "    # opening the CSV file \n",
    "    with open('weights.csv', mode ='r')as file: \n",
    "        # reading the CSV file \n",
    "        csvFile = csv.reader(file)\n",
    "        \n",
    "        i = 0\n",
    "        # displaying the contents of the CSV file \n",
    "        for lines in csvFile: \n",
    "            if i == 1:\n",
    "                layers_sizes = lines\n",
    "                i=i+1\n",
    "            elif i == 2:\n",
    "                activations = lines\n",
    "                i=i+1\n",
    "            elif i == 0:\n",
    "                wind = lines[0]\n",
    "                i=i+1\n",
    "            else:\n",
    "                ini_list.append(lines)\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                str1 = ini_list[i][j]\n",
    "                weights[i][j] = str1.strip('][').split(', ')\n",
    "        \n",
    "        test_cont(wind,activations)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WELCOME TO THE NER CODE IMPLEMENTATION\n",
      "The dataset used in the following is 'Annotated Corpus for NER, by Abhinav Walia [KAGGLE]'\n",
      "\n",
      "Do you want to train or test?\n",
      "Enter 1 for training and 2 for testing: 1\n",
      "Choose the number of hidden layers \n",
      "Min = 1 and Max = 3\n",
      "2\n",
      "Choose the activation functions for the hidden layers\n",
      "1.ReLU \n",
      "2.Sigmoid \n",
      "3.TanH \n",
      "4.Leaky ReLU \n",
      "5.Softplus \n",
      "6.ELU \n",
      "7.Linear\n",
      "Activation function for layer %d: 1\n",
      "Your Choice: 1\n",
      "Activation function for layer %d: 2\n",
      "Your Choice: 2\n",
      "Activation function for layer %d: 3\n",
      "Your Choice: 3\n",
      "\n",
      "Do you want regularization of cost function? \n",
      "Press 1 for Yes \n",
      "Press 2 for No \n",
      "1\n",
      "\n",
      "How do you want to initialise weights? \n",
      "1.Random \n",
      "2.Uniform \n",
      "1\n",
      "\n",
      "You have the window size choices of 0/1/2\n",
      "Choose your window size: 2\n",
      "\n",
      "You have the epoch choices between 0-100\n",
      "Your Choice: 1\n",
      "LAYERS SIZES [257, 50, 50, 6]\n",
      ".\n",
      "Cost after epoch: 1.791624\n",
      ".\n",
      "Cost after epoch: 1.791380\n",
      ".\n",
      "Cost after epoch: 1.791141\n",
      ".\n",
      "Cost after epoch: 1.790903\n",
      ".\n",
      "Cost after epoch: 1.790661\n",
      ".\n",
      "Cost after epoch: 1.790511\n",
      ".\n",
      "Cost after epoch: 1.790151\n",
      ".\n",
      "Cost after epoch: 1.789790\n",
      ".\n",
      "Cost after epoch: 1.789423\n",
      ".\n",
      "Cost after epoch: 1.789045\n",
      ".\n",
      "Cost after epoch: 1.789839\n",
      ".\n",
      "Cost after epoch: 1.789470\n",
      ".\n",
      "Cost after epoch: 1.789076\n",
      ".\n",
      "Cost after epoch: 1.788647\n",
      ".\n",
      "Cost after epoch: 1.788172\n",
      ".\n",
      "Cost after epoch: 1.790051\n",
      ".\n",
      "Cost after epoch: 1.789621\n",
      ".\n",
      "Cost after epoch: 1.789145\n",
      ".\n",
      "Cost after epoch: 1.788611\n",
      ".\n",
      "Cost after epoch: 1.788004\n",
      ".\n",
      "Cost after epoch: 1.786835\n",
      ".\n",
      "Cost after epoch: 1.786101\n",
      ".\n",
      "Cost after epoch: 1.785233\n",
      ".\n",
      "Cost after epoch: 1.784221\n",
      ".\n",
      "Cost after epoch: 1.783065\n",
      ".\n",
      "Cost after epoch: 1.783622\n",
      ".\n",
      "Cost after epoch: 1.782299\n",
      ".\n",
      "Cost after epoch: 1.780938\n",
      ".\n",
      "Cost after epoch: 1.779612\n",
      ".\n",
      "Cost after epoch: 1.778390\n",
      ".\n",
      "Cost after epoch: 1.776925\n",
      ".\n",
      "Cost after epoch: 1.775589\n",
      ".\n",
      "Cost after epoch: 1.774413\n",
      ".\n",
      "Cost after epoch: 1.773387\n",
      ".\n",
      "Cost after epoch: 1.772488\n",
      ".\n",
      "Cost after epoch: 1.778090\n",
      ".\n",
      "Cost after epoch: 1.777667\n",
      ".\n",
      "Cost after epoch: 1.777277\n",
      ".\n",
      "Cost after epoch: 1.776909\n",
      ".\n",
      "Cost after epoch: 1.776560\n",
      ".\n",
      "Cost after epoch: 1.774706\n",
      ".\n",
      "Cost after epoch: 1.773798\n",
      ".\n",
      "Cost after epoch: 1.772989\n",
      ".\n",
      "Cost after epoch: 1.772267\n",
      ".\n",
      "Cost after epoch: 1.771620\n",
      ".\n",
      "Cost after epoch: 1.778537\n",
      ".\n",
      "Cost after epoch: 1.778280\n",
      ".\n",
      "Cost after epoch: 1.778044\n",
      ".\n",
      "Cost after epoch: 1.777823\n",
      ".\n",
      "Cost after epoch: 1.777617\n",
      ".\n",
      "Cost after epoch: 1.775043\n",
      ".\n",
      "Cost after epoch: 1.772819\n",
      ".\n",
      "Cost after epoch: 1.770911\n",
      ".\n",
      "Cost after epoch: 1.769265\n",
      ".\n",
      "Cost after epoch: 1.767840\n",
      ".\n",
      "Cost after epoch: 1.767675\n",
      ".\n",
      "Cost after epoch: 1.765455\n",
      ".\n",
      "Cost after epoch: 1.763584\n",
      ".\n",
      "Cost after epoch: 1.762002\n",
      ".\n",
      "Cost after epoch: 1.760662\n",
      ".\n",
      "Cost after epoch: 1.726184\n",
      ".\n",
      "Cost after epoch: 1.709230\n",
      ".\n",
      "Cost after epoch: 1.694628\n",
      ".\n",
      "Cost after epoch: 1.682440\n",
      ".\n",
      "Cost after epoch: 1.672515\n",
      ".\n",
      "Cost after epoch: 1.755225\n",
      ".\n",
      "Cost after epoch: 1.756428\n",
      ".\n",
      "Cost after epoch: 1.757060\n",
      ".\n",
      "Cost after epoch: 1.757287\n",
      ".\n",
      "Cost after epoch: 1.757245\n",
      ".\n",
      "Cost after epoch: 1.761453\n",
      ".\n",
      "Cost after epoch: 1.761518\n",
      ".\n",
      "Cost after epoch: 1.761495\n",
      ".\n",
      "Cost after epoch: 1.761412\n",
      ".\n",
      "Cost after epoch: 1.761286\n",
      ".\n",
      "Cost after epoch: 1.766361\n",
      ".\n",
      "Cost after epoch: 1.766147\n",
      ".\n",
      "Cost after epoch: 1.765917\n",
      ".\n",
      "Cost after epoch: 1.765680\n",
      ".\n",
      "Cost after epoch: 1.765439\n",
      ".\n",
      "Cost after epoch: 1.779276\n",
      ".\n",
      "Cost after epoch: 1.778498\n",
      ".\n",
      "Cost after epoch: 1.777767\n",
      ".\n",
      "Cost after epoch: 1.777082\n",
      ".\n",
      "Cost after epoch: 1.776441\n",
      ".\n",
      "Cost after epoch: 1.765376\n",
      ".\n",
      "Cost after epoch: 1.763569\n",
      ".\n",
      "Cost after epoch: 1.762007\n",
      ".\n",
      "Cost after epoch: 1.760657\n",
      ".\n",
      "Cost after epoch: 1.759490\n",
      ".\n",
      "Cost after epoch: 1.775933\n",
      ".\n",
      "Cost after epoch: 1.774284\n",
      ".\n",
      "Cost after epoch: 1.772920\n",
      ".\n",
      "Cost after epoch: 1.771771\n",
      ".\n",
      "Cost after epoch: 1.770786\n",
      ".\n",
      "Cost after epoch: 1.770073\n",
      ".\n",
      "Cost after epoch: 1.769765\n",
      ".\n",
      "Cost after epoch: 1.769454\n",
      ".\n",
      "Cost after epoch: 1.769143\n",
      ".\n",
      "Cost after epoch: 1.768834\n",
      ".\n",
      "Cost after epoch: 1.775339\n",
      ".\n",
      "Cost after epoch: 1.774263\n",
      ".\n",
      "Cost after epoch: 1.773278\n",
      ".\n",
      "Cost after epoch: 1.772378\n",
      ".\n",
      "Cost after epoch: 1.771557\n",
      ".\n",
      "Cost after epoch: 1.782691\n",
      ".\n",
      "Cost after epoch: 1.782538\n",
      ".\n",
      "Cost after epoch: 1.782391\n",
      ".\n",
      "Cost after epoch: 1.782249\n",
      ".\n",
      "Cost after epoch: 1.782111\n",
      ".\n",
      "Cost after epoch: 1.771461\n",
      ".\n",
      "Cost after epoch: 1.770306\n",
      ".\n",
      "Cost after epoch: 1.769265\n",
      ".\n",
      "Cost after epoch: 1.768321\n",
      ".\n",
      "Cost after epoch: 1.767464\n",
      ".\n",
      "Cost after epoch: 1.763699\n",
      ".\n",
      "Cost after epoch: 1.761765\n",
      ".\n",
      "Cost after epoch: 1.760042\n",
      ".\n",
      "Cost after epoch: 1.758506\n",
      ".\n",
      "Cost after epoch: 1.757134\n",
      ".\n",
      "Cost after epoch: 1.752540\n",
      ".\n",
      "Cost after epoch: 1.750370\n",
      ".\n",
      "Cost after epoch: 1.748446\n",
      ".\n",
      "Cost after epoch: 1.746731\n",
      ".\n",
      "Cost after epoch: 1.745195\n",
      ".\n",
      "Cost after epoch: 1.762402\n",
      ".\n",
      "Cost after epoch: 1.762014\n",
      ".\n",
      "Cost after epoch: 1.761583\n",
      ".\n",
      "Cost after epoch: 1.761130\n",
      ".\n",
      "Cost after epoch: 1.760671\n",
      ".\n",
      "Cost after epoch: 1.744769\n",
      ".\n",
      "Cost after epoch: 1.733124\n",
      ".\n",
      "Cost after epoch: 1.723321\n",
      ".\n",
      "Cost after epoch: 1.715156\n",
      ".\n",
      "Cost after epoch: 1.708395\n",
      ".\n",
      "Cost after epoch: 1.742214\n",
      ".\n",
      "Cost after epoch: 1.741999\n",
      ".\n",
      "Cost after epoch: 1.741641\n",
      ".\n",
      "Cost after epoch: 1.741172\n",
      ".\n",
      "Cost after epoch: 1.740618\n",
      ".\n",
      "Cost after epoch: 1.758332\n",
      ".\n",
      "Cost after epoch: 1.758874\n",
      ".\n",
      "Cost after epoch: 1.759183\n",
      ".\n",
      "Cost after epoch: 1.759309\n",
      ".\n",
      "Cost after epoch: 1.759293\n",
      ".\n",
      "Cost after epoch: 1.759541\n",
      ".\n",
      "Cost after epoch: 1.757820\n",
      ".\n",
      "Cost after epoch: 1.756161\n",
      ".\n",
      "Cost after epoch: 1.754566\n",
      ".\n",
      "Cost after epoch: 1.753033\n",
      ".\n",
      "Cost after epoch: 1.761184\n",
      ".\n",
      "Cost after epoch: 1.758821\n",
      ".\n",
      "Cost after epoch: 1.756713\n",
      ".\n",
      "Cost after epoch: 1.754817\n",
      ".\n",
      "Cost after epoch: 1.753094\n",
      ".\n",
      "Cost after epoch: 1.738247\n",
      ".\n",
      "Cost after epoch: 1.735376\n",
      ".\n",
      "Cost after epoch: 1.732750\n",
      ".\n",
      "Cost after epoch: 1.730334\n",
      ".\n",
      "Cost after epoch: 1.728094\n",
      ".\n",
      "Cost after epoch: 1.755884\n",
      ".\n",
      "Cost after epoch: 1.755460\n",
      ".\n",
      "Cost after epoch: 1.754979\n",
      ".\n",
      "Cost after epoch: 1.754466\n",
      ".\n",
      "Cost after epoch: 1.753921\n",
      ".\n",
      "Cost after epoch: 1.729670\n",
      ".\n",
      "Cost after epoch: 1.717673\n",
      ".\n",
      "Cost after epoch: 1.708051\n",
      ".\n",
      "Cost after epoch: 1.700403\n",
      ".\n",
      "Cost after epoch: 1.694348\n",
      ".\n",
      "Cost after epoch: 1.748172\n",
      ".\n",
      "Cost after epoch: 1.748754\n",
      ".\n",
      "Cost after epoch: 1.749179\n",
      ".\n",
      "Cost after epoch: 1.749415\n",
      ".\n",
      "Cost after epoch: 1.749475\n",
      ".\n",
      "Cost after epoch: 1.744515\n",
      ".\n",
      "Cost after epoch: 1.742391\n",
      ".\n",
      "Cost after epoch: 1.740425\n",
      ".\n",
      "Cost after epoch: 1.738572\n",
      ".\n",
      "Cost after epoch: 1.736824\n",
      ".\n",
      "Cost after epoch: 1.729894\n",
      ".\n",
      "Cost after epoch: 1.727369\n",
      ".\n",
      "Cost after epoch: 1.725114\n",
      ".\n",
      "Cost after epoch: 1.723028\n",
      ".\n",
      "Cost after epoch: 1.721061\n",
      ".\n",
      "Cost after epoch: 1.730566\n",
      ".\n",
      "Cost after epoch: 1.725202\n",
      ".\n",
      "Cost after epoch: 1.720640\n",
      ".\n",
      "Cost after epoch: 1.716694\n",
      ".\n",
      "Cost after epoch: 1.713235\n",
      ".\n",
      "Cost after epoch: 1.724428\n",
      ".\n",
      "Cost after epoch: 1.723440\n",
      ".\n",
      "Cost after epoch: 1.722271\n",
      ".\n",
      "Cost after epoch: 1.720924\n",
      ".\n",
      "Cost after epoch: 1.719454\n",
      ".\n",
      "Cost after epoch: 1.707506\n",
      ".\n",
      "Cost after epoch: 1.704377\n",
      ".\n",
      "Cost after epoch: 1.701489\n",
      ".\n",
      "Cost after epoch: 1.698785\n",
      ".\n",
      "Cost after epoch: 1.696201\n",
      ".\n",
      "Cost after epoch: 1.729979\n",
      ".\n",
      "Cost after epoch: 1.729979\n",
      ".\n",
      "Cost after epoch: 1.729608\n",
      ".\n",
      "Cost after epoch: 1.729015\n",
      ".\n",
      "Cost after epoch: 1.728279\n",
      ".\n",
      "Cost after epoch: 1.703548\n",
      ".\n",
      "Cost after epoch: 1.697815\n",
      ".\n",
      "Cost after epoch: 1.693558\n",
      ".\n",
      "Cost after epoch: 1.690229\n",
      ".\n",
      "Cost after epoch: 1.687562\n",
      ".\n",
      "Cost after epoch: 1.679847\n",
      ".\n",
      "Cost after epoch: 1.675672\n",
      ".\n",
      "Cost after epoch: 1.671989\n",
      ".\n",
      "Cost after epoch: 1.668670\n",
      ".\n",
      "Cost after epoch: 1.665593\n",
      ".\n",
      "Cost after epoch: 1.686026\n",
      ".\n",
      "Cost after epoch: 1.686626\n",
      ".\n",
      "Cost after epoch: 1.686641\n",
      ".\n",
      "Cost after epoch: 1.686230\n",
      ".\n",
      "Cost after epoch: 1.685556\n",
      ".\n",
      "Cost after epoch: 1.704642\n",
      ".\n",
      "Cost after epoch: 1.705598\n",
      ".\n",
      "Cost after epoch: 1.705962\n",
      ".\n",
      "Cost after epoch: 1.705947\n",
      ".\n",
      "Cost after epoch: 1.705701\n",
      ".\n",
      "Cost after epoch: 1.695092\n",
      ".\n",
      "Cost after epoch: 1.691061\n",
      ".\n",
      "Cost after epoch: 1.687667\n",
      ".\n",
      "Cost after epoch: 1.684831\n",
      ".\n",
      "Cost after epoch: 1.682350\n",
      ".\n",
      "Cost after epoch: 1.691234\n",
      ".\n",
      "Cost after epoch: 1.688088\n",
      ".\n",
      "Cost after epoch: 1.685366\n",
      ".\n",
      "Cost after epoch: 1.682919\n",
      ".\n",
      "Cost after epoch: 1.680706\n",
      ".\n",
      "Cost after epoch: 1.660894\n",
      ".\n",
      "Cost after epoch: 1.660085\n",
      ".\n",
      "Cost after epoch: 1.659220\n",
      ".\n",
      "Cost after epoch: 1.658321\n",
      ".\n",
      "Cost after epoch: 1.657435\n",
      ".\n",
      "Cost after epoch: 1.660466\n",
      ".\n",
      "Cost after epoch: 1.658388\n",
      ".\n",
      "Cost after epoch: 1.656437\n",
      ".\n",
      "Cost after epoch: 1.654633\n",
      ".\n",
      "Cost after epoch: 1.652958\n",
      ".\n",
      "Cost after epoch: 1.648768\n",
      ".\n",
      "Cost after epoch: 1.645845\n",
      ".\n",
      "Cost after epoch: 1.643439\n",
      ".\n",
      "Cost after epoch: 1.641376\n",
      ".\n",
      "Cost after epoch: 1.639520\n",
      ".\n",
      "Cost after epoch: 1.679449\n",
      ".\n",
      "Cost after epoch: 1.675219\n",
      ".\n",
      "Cost after epoch: 1.670515\n",
      ".\n",
      "Cost after epoch: 1.665703\n",
      ".\n",
      "Cost after epoch: 1.661013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "Cost after epoch: 1.658071\n",
      ".\n",
      "Cost after epoch: 1.648816\n",
      ".\n",
      "Cost after epoch: 1.642319\n",
      ".\n",
      "Cost after epoch: 1.637457\n",
      ".\n",
      "Cost after epoch: 1.633615\n",
      ".\n",
      "Cost after epoch: 1.665179\n",
      ".\n",
      "Cost after epoch: 1.666078\n",
      ".\n",
      "Cost after epoch: 1.666652\n",
      ".\n",
      "Cost after epoch: 1.666745\n",
      ".\n",
      "Cost after epoch: 1.666446\n",
      ".\n",
      "Cost after epoch: 1.678947\n",
      ".\n",
      "Cost after epoch: 1.677933\n",
      ".\n",
      "Cost after epoch: 1.677028\n",
      ".\n",
      "Cost after epoch: 1.676213\n",
      ".\n",
      "Cost after epoch: 1.675487\n",
      ".\n",
      "Cost after epoch: 1.662016\n",
      ".\n",
      "Cost after epoch: 1.658539\n",
      ".\n",
      "Cost after epoch: 1.655435\n",
      ".\n",
      "Cost after epoch: 1.652638\n",
      ".\n",
      "Cost after epoch: 1.650096\n",
      ".\n",
      "Cost after epoch: 1.667581\n",
      ".\n",
      "Cost after epoch: 1.666988\n",
      ".\n",
      "Cost after epoch: 1.666510\n",
      ".\n",
      "Cost after epoch: 1.666092\n",
      ".\n",
      "Cost after epoch: 1.665723\n",
      ".\n",
      "Cost after epoch: 1.654006\n",
      ".\n",
      "Cost after epoch: 1.650582\n",
      ".\n",
      "Cost after epoch: 1.648057\n",
      ".\n",
      "Cost after epoch: 1.646063\n",
      ".\n",
      "Cost after epoch: 1.644417\n",
      ".\n",
      "Cost after epoch: 1.650896\n",
      ".\n",
      "Cost after epoch: 1.651521\n",
      ".\n",
      "Cost after epoch: 1.652017\n",
      ".\n",
      "Cost after epoch: 1.652283\n",
      ".\n",
      "Cost after epoch: 1.652361\n",
      ".\n",
      "Cost after epoch: 1.651150\n",
      ".\n",
      "Cost after epoch: 1.650688\n",
      ".\n",
      "Cost after epoch: 1.650257\n",
      ".\n",
      "Cost after epoch: 1.649851\n",
      ".\n",
      "Cost after epoch: 1.649463\n",
      ".\n",
      "Cost after epoch: 1.625524\n",
      ".\n",
      "Cost after epoch: 1.620219\n",
      ".\n",
      "Cost after epoch: 1.616625\n",
      ".\n",
      "Cost after epoch: 1.614001\n",
      ".\n",
      "Cost after epoch: 1.611956\n",
      ".\n",
      "Cost after epoch: 1.623794\n",
      ".\n",
      "Cost after epoch: 1.623384\n",
      ".\n",
      "Cost after epoch: 1.622794\n",
      ".\n",
      "Cost after epoch: 1.622087\n",
      ".\n",
      "Cost after epoch: 1.621287\n",
      ".\n",
      "Cost after epoch: 1.642612\n",
      ".\n",
      "Cost after epoch: 1.639150\n",
      ".\n",
      "Cost after epoch: 1.636383\n",
      ".\n",
      "Cost after epoch: 1.634096\n",
      ".\n",
      "Cost after epoch: 1.632159\n",
      ".\n",
      "Cost after epoch: 1.662488\n",
      ".\n",
      "Cost after epoch: 1.661376\n",
      ".\n",
      "Cost after epoch: 1.659733\n",
      ".\n",
      "Cost after epoch: 1.657510\n",
      ".\n",
      "Cost after epoch: 1.655047\n",
      ".\n",
      "Cost after epoch: 1.658409\n",
      ".\n",
      "Cost after epoch: 1.649034\n",
      ".\n",
      "Cost after epoch: 1.642549\n",
      ".\n",
      "Cost after epoch: 1.637871\n",
      ".\n",
      "Cost after epoch: 1.634352\n",
      ".\n",
      "Cost after epoch: 1.641729\n",
      ".\n",
      "Cost after epoch: 1.642085\n",
      ".\n",
      "Cost after epoch: 1.642245\n",
      ".\n",
      "Cost after epoch: 1.642267\n",
      ".\n",
      "Cost after epoch: 1.642191\n",
      ".\n",
      "Cost after epoch: 1.666412\n",
      ".\n",
      "Cost after epoch: 1.666180\n",
      ".\n",
      "Cost after epoch: 1.665906\n",
      ".\n",
      "Cost after epoch: 1.665587\n",
      ".\n",
      "Cost after epoch: 1.665238\n",
      ".\n",
      "Cost after epoch: 1.655552\n",
      ".\n",
      "Cost after epoch: 1.651302\n",
      ".\n",
      "Cost after epoch: 1.648660\n",
      ".\n",
      "Cost after epoch: 1.646866\n",
      ".\n",
      "Cost after epoch: 1.645560\n",
      ".\n",
      "Cost after epoch: 1.656585\n",
      ".\n",
      "Cost after epoch: 1.656203\n",
      ".\n",
      "Cost after epoch: 1.655833\n",
      ".\n",
      "Cost after epoch: 1.655410\n",
      ".\n",
      "Cost after epoch: 1.654936\n",
      ".\n",
      "Cost after epoch: 1.641355\n",
      ".\n",
      "Cost after epoch: 1.639327\n",
      ".\n",
      "Cost after epoch: 1.637705\n",
      ".\n",
      "Cost after epoch: 1.636354\n",
      ".\n",
      "Cost after epoch: 1.635192\n",
      ".\n",
      "Cost after epoch: 1.661521\n",
      ".\n",
      "Cost after epoch: 1.661621\n",
      ".\n",
      "Cost after epoch: 1.661687\n",
      ".\n",
      "Cost after epoch: 1.661723\n",
      ".\n",
      "Cost after epoch: 1.661729\n",
      ".\n",
      "Cost after epoch: 1.661140\n",
      ".\n",
      "Cost after epoch: 1.660174\n",
      ".\n",
      "Cost after epoch: 1.659273\n",
      ".\n",
      "Cost after epoch: 1.658433\n",
      ".\n",
      "Cost after epoch: 1.657649\n",
      ".\n",
      "Cost after epoch: 1.617101\n",
      ".\n",
      "Cost after epoch: 1.612210\n",
      ".\n",
      "Cost after epoch: 1.607814\n",
      ".\n",
      "Cost after epoch: 1.603864\n",
      ".\n",
      "Cost after epoch: 1.600329\n",
      ".\n",
      "Cost after epoch: 1.610793\n",
      ".\n",
      "Cost after epoch: 1.610645\n",
      ".\n",
      "Cost after epoch: 1.610608\n",
      ".\n",
      "Cost after epoch: 1.610643\n",
      ".\n",
      "Cost after epoch: 1.610698\n",
      ".\n",
      "Cost after epoch: 1.659003\n",
      ".\n",
      "Cost after epoch: 1.658966\n",
      ".\n",
      "Cost after epoch: 1.658897\n",
      ".\n",
      "Cost after epoch: 1.658794\n",
      ".\n",
      "Cost after epoch: 1.658659\n",
      ".\n",
      "Cost after epoch: 1.607062\n",
      ".\n",
      "Cost after epoch: 1.598850\n",
      ".\n",
      "Cost after epoch: 1.594395\n",
      ".\n",
      "Cost after epoch: 1.591706\n",
      ".\n",
      "Cost after epoch: 1.589989\n",
      ".\n",
      "Cost after epoch: 1.638992\n",
      ".\n",
      "Cost after epoch: 1.637855\n",
      ".\n",
      "Cost after epoch: 1.636911\n",
      ".\n",
      "Cost after epoch: 1.636052\n",
      ".\n",
      "Cost after epoch: 1.635235\n",
      ".\n",
      "Cost after epoch: 1.659420\n",
      ".\n",
      "Cost after epoch: 1.654897\n",
      ".\n",
      "Cost after epoch: 1.650805\n",
      ".\n",
      "Cost after epoch: 1.647107\n",
      ".\n",
      "Cost after epoch: 1.643772\n",
      ".\n",
      "Cost after epoch: 1.638357\n",
      ".\n",
      "Cost after epoch: 1.634862\n",
      ".\n",
      "Cost after epoch: 1.631956\n",
      ".\n",
      "Cost after epoch: 1.629463\n",
      ".\n",
      "Cost after epoch: 1.627294\n",
      ".\n",
      "Cost after epoch: 1.655798\n",
      ".\n",
      "Cost after epoch: 1.654207\n",
      ".\n",
      "Cost after epoch: 1.652952\n",
      ".\n",
      "Cost after epoch: 1.651938\n",
      ".\n",
      "Cost after epoch: 1.651094\n",
      ".\n",
      "Cost after epoch: 1.651810\n",
      ".\n",
      "Cost after epoch: 1.648169\n",
      ".\n",
      "Cost after epoch: 1.645171\n",
      ".\n",
      "Cost after epoch: 1.642660\n",
      ".\n",
      "Cost after epoch: 1.640528\n",
      ".\n",
      "Cost after epoch: 1.623199\n",
      ".\n",
      "Cost after epoch: 1.619512\n",
      ".\n",
      "Cost after epoch: 1.616735\n",
      ".\n",
      "Cost after epoch: 1.614542\n",
      ".\n",
      "Cost after epoch: 1.612728\n",
      ".\n",
      "Cost after epoch: 1.651092\n",
      ".\n",
      "Cost after epoch: 1.651154\n",
      ".\n",
      "Cost after epoch: 1.651199\n",
      ".\n",
      "Cost after epoch: 1.651206\n",
      ".\n",
      "Cost after epoch: 1.651176\n",
      ".\n",
      "Cost after epoch: 1.617236\n",
      ".\n",
      "Cost after epoch: 1.613543\n",
      ".\n",
      "Cost after epoch: 1.610277\n",
      ".\n",
      "Cost after epoch: 1.607404\n",
      ".\n",
      "Cost after epoch: 1.604880\n",
      ".\n",
      "Cost after epoch: 1.684032\n",
      ".\n",
      "Cost after epoch: 1.675711\n",
      ".\n",
      "Cost after epoch: 1.669498\n",
      ".\n",
      "Cost after epoch: 1.664547\n",
      ".\n",
      "Cost after epoch: 1.660431\n",
      ".\n",
      "Cost after epoch: 1.625664\n",
      ".\n",
      "Cost after epoch: 1.625281\n",
      ".\n",
      "Cost after epoch: 1.624898\n",
      ".\n",
      "Cost after epoch: 1.624507\n",
      ".\n",
      "Cost after epoch: 1.624110\n",
      ".\n",
      "Cost after epoch: 1.623136\n",
      ".\n",
      "Cost after epoch: 1.620563\n",
      ".\n",
      "Cost after epoch: 1.618332\n",
      ".\n",
      "Cost after epoch: 1.616371\n",
      ".\n",
      "Cost after epoch: 1.614628\n",
      ".\n",
      "Cost after epoch: 1.598633\n",
      ".\n",
      "Cost after epoch: 1.596795\n",
      ".\n",
      "Cost after epoch: 1.595169\n",
      ".\n",
      "Cost after epoch: 1.593715\n",
      ".\n",
      "Cost after epoch: 1.592409\n",
      ".\n",
      "Cost after epoch: 1.629955\n",
      ".\n",
      "Cost after epoch: 1.628657\n",
      ".\n",
      "Cost after epoch: 1.627327\n",
      ".\n",
      "Cost after epoch: 1.626046\n",
      ".\n",
      "Cost after epoch: 1.624867\n",
      ".\n",
      "Cost after epoch: 1.625634\n",
      ".\n",
      "Cost after epoch: 1.619951\n",
      ".\n",
      "Cost after epoch: 1.615820\n",
      ".\n",
      "Cost after epoch: 1.612564\n",
      ".\n",
      "Cost after epoch: 1.609868\n",
      "TOTAL COUNT:  [2089. 3111. 3492. 2982. 4500. 1612.]\n"
     ]
    }
   ],
   "source": [
    "print(\"WELCOME TO THE NER CODE IMPLEMENTATION\")\n",
    "print(\"The dataset used in the following is 'Annotated Corpus for NER, by Abhinav Walia [KAGGLE]'\")\n",
    "print(\"\\nDo you want to train or test?\")\n",
    "tort = input(\"Enter 1 for training and 2 for testing: \")\n",
    "\n",
    "global weights\n",
    "weights = []\n",
    "if tort == \"1\":\n",
    "    train()\n",
    "elif tort == \"2\":\n",
    "    test()\n",
    "else:\n",
    "    print(\"Wrong choice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
